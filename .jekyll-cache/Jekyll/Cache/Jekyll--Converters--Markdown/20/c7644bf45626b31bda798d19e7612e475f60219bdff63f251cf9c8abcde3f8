I"r<h1 id="building-data-engineering-pipelines-in-python">Building Data Engineering Pipelines in Python</h1>

<p><br /></p>

<h2 id="1-ingesting-data">1. Ingesting Data</h2>

<h4 id="data-is-valuable">Data is valuable</h4>

<ul>
  <li>
    <p>현대 조직은, 데이터 수집이 얼마나 가치 있는지 인식하고 있습니다.</p>
  </li>
  <li>
    <p>데이터 는 내부적으로, 점점 더 많은 사람들이 사용하고 있습니다.</p>
  </li>
  <li>
    <p>회사 내 거의 모든 사람이 액세스 할 수 있고, 새로운 통찰력을 얻을 수 있습니다.</p>
  </li>
  <li>
    <p>대중을 향한 측면에서, 기업들이 더 많은 데이터를 사용할 수 있도록 하고 있습니다. (공용 API 등)</p>
  </li>
</ul>

<p><br /></p>

<h4 id="genesis-of-the-data">Genesis of the data</h4>

<ul>
  <li>
    <p>데이터의 기원은 수집 된 스트리밍 데이터와 같은 운영 체제에 있습니다.</p>
  </li>
  <li>
    <p>데이터는 구글 애널리틱스, 판매 플랫폼 등 사물인터넷 장치, 웹 세션 데이터 등으로 부터 생성됩니다.</p>
  </li>
</ul>

<p>이러한 데이터는 나중에 처리 할 수 있도록 어딘가에 저장해야 합니다.</p>

<p>오늘날 데이터의 규모와 데이터가 흐르는 속도는 우리가 “데이터 레이크” 라고 부르고 있습니다.</p>

<p><br /></p>

<h4 id="operational-data-is-stored-in-the-landing-zone">Operational data is stored in the landing zone</h4>

<p>데이터 레이크는 여러 시스템으로 구성되며 일반적으로 여러 영역으로 구성됩니다.</p>

<p>ex) 운영 체제에서 가져온 데이터는 “LANDING” 이라고 부르는 곳에서 끝납니다.</p>

<p>데이터 레이크로 부터, 데이터를 가져오는 과정을 수집이라고 부릅니다.</p>

<p><br /></p>

<h4 id="cleaned-data-prevents-rework">Cleaned data prevents rework</h4>

<p>사람들은 데이터 레이크 위에 다양한 서비스를 구축합니다. ( 예측 알고리즘, A/B 테스트 용 대시보드)</p>

<p>이러한 서비스 중 상당수는 데이터에 변환을 적용해야 합니다.</p>

<p>변환의 중복을 방지하기 위해 , Landing zone 은 청소되어 청정 구역에 저장됩니다.</p>

<p><br /></p>

<h4 id="the-business-layer-provides-most-insights">The business layer provides most insights</h4>

<p>마지막으로, 사용 사례별로이 깨끗한 데이터에 몇 가지 특수 변환이 적용됩니다.</p>

<p>ex) 이탈 할 가능성이 있는 고객을 예측하는 것은 일반적인 비지니스 사용 사례</p>

<p>정리 된, 데이터 세트로 구성된 데이터 세트를 기계학습 알고리즘에 적용합니다.</p>

<p><br /></p>

<h4 id="pipelines-move-data-from-one-zone-to-another">Pipelines move data from one zone to another</h4>

<p>한 영역에서 다른 영역으로 데이터를 이동하고, 그 과정에서 데이터를 변환하기 위해 사람들은 데이터 파이프 파인을 구축합니다.</p>

<p>파이프 라인은 파일과 같은 외부 이벤트에 의해 트리거 될 수 있습니다. (특정 위치, 일정 시간 또는 수동으로 저장)</p>

<p>일반적으로 대규모 배치로 데이터를 처리하는 파이프 라인은, 야간과 같은 일정에 따라 트리거됩니다.</p>

<p>이러한 파이프 라인을 추출, 변환, 로드 파이프라인 또는 ETL 파이프 라인이라고 합니다.</p>

<p>이들을 잘 관리하기 위해서, 좋은 도구를 이용하는 데, 그것이 Airflow입니다.</p>

<p><br /></p>

<h4 id="singers-core-concepts">Singer’s core concepts</h4>

<p>Singer의 목표는 데이터를 이동하는 스크립트를 작성하기 위한 오픈 소스 표준이 되는 것입니다.</p>

<p>데이터 추출 스크립트 및 데이터로드 스크립트는 표준 출력을 통해 표준 JSON 기반 데이터 형식을 사용하여 통신 해야 합니다.</p>

<p>Singer는 사양이므로 “탭” 이라고 하는 추출 스크립트 및 “타켓” 이라고하는 로딩 스크립트는 모든 프로그래밍 언어로 작성할 수 있습니다.</p>

<p>데이터를 한 위치에서 다른 위치로 이동하는 작은 데이터 파이프라인을 만들기 위해 쉽게 혼합 및 일치 시킬수 있습니다.</p>

<p>탭과 타겟은, schema, state, record 특정 스크림으로 전송 및 읽기</p>

<p>스트림은 메시지를 보내는 이름으로 지정된 가상 위치이며 다운 스트림 위치에서 선택 할 수 있습니다.</p>

<p>주제에 따라 다른 스트림을 사용하여 데이터를 분할 할 수 있습니다.</p>

<p>ex) 오류 메시지는 오류 스트림으로 이동하고, 다른 데이터베이스 테이블의 데이터도 다른 스트림으로 이동</p>

<p><br /></p>

<h4 id="describing-the-data-through-its-schema">Describing the data through its schema</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">columns</span> <span class="o">=</span> <span class="p">{</span><span class="s">"id"</span><span class="p">,</span><span class="s">"name"</span><span class="p">,</span><span class="s">"age"</span><span class="p">,</span><span class="s">"has_children"</span><span class="p">}</span>
<span class="n">users</span> <span class="o">=</span> <span class="p">{(</span><span class="mi">1</span><span class="p">,</span> <span class="s">"Adrian"</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span><span class="bp">False</span><span class="p">),</span>
        <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s">"Ruanne"</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="bp">False</span><span class="p">),</span>
        <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s">"Hillary"</span><span class="p">,</span> <span class="mi">29</span><span class="p">,</span> <span class="bp">True</span><span class="p">)}</span>
<span class="n">json_schema</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s">"properties"</span><span class="p">:</span> <span class="p">{</span><span class="s">"age"</span><span class="p">:</span> <span class="p">{</span><span class="s">"maximum"</span><span class="p">:</span> <span class="mi">130</span><span class="p">,</span>
                        <span class="s">"minimum"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                        <span class="s">"type"</span><span class="p">:</span> <span class="s">"integer"</span><span class="p">},</span>
                <span class="s">"has_children"</span><span class="p">:</span> <span class="p">{</span><span class="s">"type"</span><span class="p">:</span> <span class="s">"boolean"</span><span class="p">},</span>
                <span class="s">"id"</span><span class="p">:</span> <span class="p">{</span><span class="s">"type"</span><span class="p">:</span> <span class="s">"integer"</span><span class="p">},</span>
                <span class="s">"name"</span><span class="p">:</span> <span class="p">{</span><span class="s">"type"</span><span class="p">:</span> <span class="s">"string"</span><span class="p">}},</span>
  <span class="s">"$id"</span><span class="p">:</span> <span class="s">"http://yourdomain.com/schemas/my_user_schema.json"</span><span class="p">,</span>
  <span class="s">"$schema"</span><span class="p">:</span> <span class="s">"http://json-schema.org/draft-07/schema#"</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Singer 사양을 사용하면, 먼저 스키마를 지정 하여 데이터를 설명합니다.</p>

<p>스키마는 유효한 JSON 스키마로 제공되어야 합니다.</p>

<p>구조화 된 데이터에 주석을 달고 유효성을 검사 할수 있는 사양입니다. 각 속성 또는 필드의 데이터 유형을 지정합니다.</p>

<p>나이가 정수 값이어야 한다는 제약 혹은, 최소값, 최댓값을 부과할 수 도 있습니다.</p>

<p>$id, $schema를 통해 스키마를 고유하게 지정할 수 있습니다. 또한, 조작하고 사용중인 JSON 스키마 버전을 다른 사용자에게 알려줍니다. (이는 선택 옵션이지만, 프로덕션 등급의 코드에서 적극 권장됩니다. )</p>

<p><br /></p>

<h4 id="describing-the-data-through-its-schema-1">Describing the data through its schema</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="kn">import</span> <span class="nn">singer</span>
 <span class="n">singer</span><span class="p">.</span><span class="n">write_schema</span><span class="p">(</span><span class="n">schema</span><span class="o">=</span><span class="n">json_schema</span><span class="p">,</span>
                    <span class="n">stream_name</span><span class="o">=</span><span class="s">"DC_employees"</span><span class="p">,</span>
                    <span class="n">key_properties</span><span class="o">=</span><span class="p">[</span><span class="s">"id"</span><span class="p">])</span>
</code></pre></div></div>

<p>Singer 라이브러리에 JSON 스키마에서, 스키마 메시지를 만들도록 지시 할 수 있습니다.</p>

<p>Singer의 “write_schema” 함수를 호출하여, 앞서 정의한 “json_schema”를 전달합니다.</p>

<p>“Stream_name”을 사용하여 이 메시지가 속한 스트림의 이름을 지정합니다. ( 무엇이든 가능 )</p>

<p>함께 속한 데이터는 동일한 스트림으로 전송되어야 합니다.</p>

<p>“key_properites” 속성은 다음 목록과 같아야합니다.</p>

<ul>
  <li>이 스트림의 레코드에 대한 기본 키를 구성하는 문자열 (주차장에서 자동차의 데이터를 처리하는 경우, 번호판이나, 대리 키가 될 수 있습니다.)</li>
  <li>기본 키가 없으면 빈 목록을 지정</li>
</ul>

<p><br /></p>

<h4 id="serializing-json">Serializing JSON</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">json</span>

<span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">json_schema</span><span class="p">[</span><span class="s">"properties"</span><span class="p">][</span><span class="s">"age"</span><span class="p">])</span>
</code></pre></div></div>

<p>JSON은 Singer뿐만 아니라, 다른 여러 곳에서도 일반적인 형식입니다.</p>

<p>Python은 JSON과 함께 작동하는 json모듈을 제공합니다.</p>

<p>JSON으로 직렬화 된 코드의 객체를 가져 오려면, json dump를 사용 하면 됩니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">"foo.json"</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span><span class="s">"w"</span><span class="p">)</span> <span class="k">as</span> <span class="n">fh</span><span class="p">:</span>
  <span class="n">json</span><span class="p">.</span><span class="n">dump</span><span class="p">(</span><span class="n">obj</span><span class="o">=</span><span class="n">json_schema</span><span class="p">,</span> <span class="n">fp</span><span class="o">=</span><span class="n">fh</span><span class="p">)</span>
</code></pre></div></div>

<p>첫번쨰 코드는, 단순히 객체를 문자열로 변환하는 반면, 두번째 코드는 동일한 문자열을 파일에 씁니다.</p>

<p><br /></p>

<h4 id="streaming-record-messages">Streaming record messages</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">columns</span> <span class="o">=</span> <span class="p">(</span><span class="s">"id"</span><span class="p">,</span> <span class="s">"name"</span><span class="p">,</span> <span class="s">"age"</span><span class="p">,</span> <span class="s">"has_children"</span><span class="p">)</span>
<span class="n">user</span> <span class="o">=</span> <span class="p">{(</span><span class="mi">1</span><span class="p">,</span> <span class="s">"Adrian"</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="bp">False</span><span class="p">),</span>
        <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s">"Ruanne"</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="bp">False</span><span class="p">),</span>
       <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s">"Hillary"</span><span class="p">,</span> <span class="mi">29</span><span class="p">,</span> <span class="bp">True</span><span class="p">)}</span>

<span class="n">singer</span><span class="p">.</span><span class="n">write_record</span><span class="p">(</span><span class="n">stream_name</span><span class="o">=</span><span class="s">"DC_employees"</span><span class="p">,</span>
                   <span class="n">record</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="n">user</span><span class="p">.</span><span class="n">pop</span><span class="p">())))</span>
<span class="o">-----------------------------------------------------</span>
<span class="n">fixed_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s">"type"</span><span class="p">:</span> <span class="s">"RECORD"</span><span class="p">,</span> <span class="s">"stream"</span><span class="p">:</span> <span class="s">"DC_employee"</span><span class="p">}</span>
<span class="n">recored_msg</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">fixed_dict</span><span class="p">,</span> <span class="s">"record"</span><span class="p">:</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="n">users</span><span class="p">.</span><span class="n">pop</span><span class="p">()))}</span>
<span class="k">print</span><span class="p">(</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">record_msg</span><span class="p">))</span>
</code></pre></div></div>

<p>이러한 사용자를 Singer Record 메세지로 변환하기 위해 write_record함수를 호출합니다.</p>

<p>stream_name은 이전에 스키마 메시지에서 지정한 스트림과 일치해아합니다.</p>

<p>(그렇지 않으면, 레코드가 무시됩니다.)</p>

<p>두 개의 키가 더있는 또다른 딕셔너리는 타입와 스트림입니다.</p>

<p>언패킹 연산자를 사용하면, 우아하게 수행 가능.</p>

<p><br /></p>

<h4 id="chaining-taps-and-targets">Chaining taps and targets</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Module: my_tap.py
</span><span class="kn">import</span> <span class="nn">singer</span>

<span class="n">singer</span><span class="p">.</span><span class="n">write_schema</span><span class="p">(</span><span class="n">stream_name</span><span class="o">=</span><span class="s">"foo"</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="p">..)</span>
<span class="n">singer</span><span class="p">.</span><span class="n">write_records</span><span class="p">(</span><span class="n">steram</span><span class="o">=</span><span class="s">"foo"</span><span class="p">,</span> <span class="n">records</span><span class="o">=</span><span class="p">..)</span>
</code></pre></div></div>

<p>write_schema 와 write_record를 결합하는 경우 함수를 사용하면 JSON객체를 stdout에 인쇄하는 python 모듈이 있습니다.</p>

<p>이러한 메시지를 구문 분석 할 수 있는 Singer 대상도 있는 경우 전체 수집 파이프 라인이 있습니다. ( <code class="language-plaintext highlighter-rouge">write_record</code> 가 아닌 <code class="language-plaintext highlighter-rouge">write_records</code>사용)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">python</span> <span class="n">my_tap</span><span class="p">.</span><span class="n">py</span> <span class="o">|</span> <span class="n">target</span><span class="o">-</span><span class="n">csv</span>
<span class="n">python</span> <span class="n">my_tap</span><span class="p">.</span><span class="n">py</span> <span class="o">|</span> <span class="n">target</span><span class="o">-</span><span class="n">csv</span> <span class="o">--</span><span class="n">config</span> <span class="n">userconfig</span><span class="p">.</span><span class="n">cfg</span>
</code></pre></div></div>

<p>파이썬 패키지 색인에서 사용할 수 있는 target_csv는 JSON라인에서 CSV파일을 만드는 것입니다.</p>

<p>CSV파일은 이것을 실행하는 동일한 디렉토리에 만들어집니다.</p>

<p>구성 파일을 제공하여 달리 구성하지 않는 명령</p>

<p>파이썬 인터프리터로 코드를 파싱하여 탭을 실행하는 것을 막을 수는 없습니다. 하지만 일반적으로 적절하게 패키징 된 탭과 타켓을 찾을 수 있으므로, 이와 같이 직접 호출하여 사용합니다.</p>

<p><br /></p>

<h4 id="modular-ingestion-pipelines">Modular ingestion pipelines</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">my</span><span class="o">-</span><span class="n">packaged</span><span class="o">-</span><span class="n">tap</span> <span class="o">|</span> <span class="n">target</span><span class="o">-</span><span class="n">csv</span>
<span class="n">my</span><span class="o">-</span><span class="n">packeged</span><span class="o">-</span><span class="n">tap</span> <span class="o">|</span> <span class="n">target</span><span class="o">-</span><span class="n">google</span><span class="o">-</span><span class="n">sheets</span>
<span class="n">my</span><span class="o">-</span><span class="n">packaged</span><span class="o">-</span><span class="n">tap</span> <span class="o">|</span> <span class="n">target</span><span class="o">-</span><span class="n">postgresql</span> <span class="o">--</span><span class="n">config</span> <span class="n">conf</span><span class="p">.</span><span class="n">json</span>

<span class="n">tap</span><span class="o">-</span><span class="n">custom</span><span class="o">-</span><span class="n">google</span><span class="o">-</span><span class="n">scraper</span> <span class="o">|</span> <span class="n">target</span><span class="o">-</span><span class="n">postgresql</span> <span class="o">--</span><span class="n">config</span> <span class="n">headlines</span><span class="p">.</span><span class="n">json</span>
</code></pre></div></div>

<p>파이썬의 csv모듈은 기능을 모듈성을 제공합니다.</p>

<p>각 탭 또는 대상은 한 가지를 매우 잘 수행하도록 설계되었습니다. 구성 파일을 통해서 쉽게 구성 할 수 있습니다.</p>

<p>표준화 된 중간 형식으로 작업하면 target-csv를 쉽게 교체할 수 있습니다. ( 구글시트 , Posrgresql 등)</p>

<p>즉, 많은 코드를 작성할 필요없이, 탭을 선택하고  의도 한 소스 및 대상과 일치하는 대상을 검색하면 준비가 완료됩니다.</p>

<p><br /></p>

<h4 id="keeping-track-with-state-messages">Keeping track with state messages</h4>

<p>Singer의 state의 메시지는, 일반적으로 상태를 추적하는데 사용되며, 이는 특정 시점에 있는 방식입니다.</p>

<p>이것은 일반적으로 프로세스에 대한 일종의 기억입니다.</p>

<table>
  <thead>
    <tr>
      <th>id</th>
      <th>name</th>
      <th>last_updated_on</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>Adrian</td>
      <td>2019-06-14T14:00:04.000+02:00</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Ruanne</td>
      <td>2019-06-16T18:33:21.000+02:00</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Hillary</td>
      <td>2019-06-14T10:05:12.000+02:00</td>
    </tr>
  </tbody>
</table>

<p>매일 정오, 현지 시간에 데이터베이스에 새 레코드만 추출한다고 가정하면 가장 쉬운 방법은 가장 많이 발생하는 항목을 추적하는 것 입니다.</p>

<p><code class="language-plaintext highlighter-rouge">last_updated_on</code>값을 입력하고, 탭을 성고적으로 실행 한 후 상태 메시지로 보냅니다. 이후 동일한 메시지를 재사용하여 이전 상태 이후에 업데이트 된 레코드만 추출합니다.</p>

<p><code class="language-plaintext highlighter-rouge">write_state</code>함수를 사용하여 이러한 상태 메시지를 보냅니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">singer</span><span class="p">.</span><span class="n">write_state</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="p">{</span><span class="s">"max-last-updated-on"</span><span class="p">:</span> <span class="n">some_variable</span><span class="p">})</span>
</code></pre></div></div>

<p>값 필드는 자유 형식이며 동일한 탭에서만 사용할 수 있습니다.</p>

<p><br /></p>

<h2 id="2-creating-a-data-transformation-pipeline-with-pyspark">2. Creating a data transformation pipeline with PySpark</h2>

<p><br /></p>

<h4 id="what-is-spark">What is spark</h4>

<p>스파크는 대량의 데이터를 처리하기 위한 분석 엔진입니다.</p>

<p>스파크 코어는 서로 다르지만, 관련된 4개의 라이브러리를 지원합니다.</p>

<ul>
  <li>Spark SQL</li>
  <li>Spark Streaming</li>
  <li>MLlib</li>
  <li>GraphX</li>
</ul>

<p>이러한 라이브러리는 동일한 애플리케이션에서 원활하게 결합 될 수 있습니다. Spark는 랩탑 및 클라우드의 독립 실행 형을 포함하여 모든 곳에서 실행됩니다.</p>

<p>또한 Python과 같이 여러 프로그래밍 언어로 존재하며 종종 PySpark라고도 불립니다.</p>

<p><br /></p>

<h4 id="when-to-use-spark">When to use Spark</h4>

<p>스파크는 대규모 데이터를 처리하는 경우 유용한 프레임 워크로, 수십억 개의 데이터 세트 크기로 매우 잘 확장됩니다.</p>

<ul>
  <li>여러 시스템에서 실행을 병렬화하여 기록합니다.</li>
  <li>데이터 과학자가 사용하는 노트북과 같은 대화형 분석에서도 사용 가능합니다. ( 대화식으로 데이터를 탐색하고, 데이터에 대한 가설을 신속하게 검증하고, 결과를 심층 분석합니다. )</li>
  <li>기계 학습 알고리즘 및 기능 모음을 제공합니다. (기능 구축, 모델 학습, 새 데이터에 대한 기존 모델 평가 등)</li>
</ul>

<p>스파크를 사용하면 안되는 경우</p>

<ul>
  <li>데이터가 거의 없을 때 : 스파크는 오버 헤드를 추가하여, 소량의 데이터에 대해 비효율적이고 느립니다.</li>
  <li>간단한 필터를 적용하기 만한다면, 스파크의 풍부함이 과잉이 됩니다.</li>
</ul>

<p><br /></p>

<h4 id="business-case-finding-the-perfect-diaper">Business case: finding the perfect diaper</h4>

<ul>
  <li>편안함과 같은 질적 측면</li>
  <li>가격과 같은 양적인 측면</li>
</ul>

<p>마케팅 부서는 이미 다음과 같은 원시 데이터 소스를 준비</p>

<ul>
  <li>가격 ( 매장별 해당 모델 가격 데이터 )</li>
  <li>평점 ( 사용자별 해당 모델 평점 데이터 )</li>
</ul>

<p><br /></p>

<h4 id="starting-the-spark-analytics-engine">Starting the Spark analytics engine</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</code></pre></div></div>

<p>PySpark로 데이터 세트를 로드하려면, SparkSession을 만들어야 합니다. ( 스파크 버전2 이상 )</p>

<p>생성 시 실행 환경이 구성됩니다.</p>

<p><br /></p>

<h4 id="reading-a-csv-file">Reading a CSV file</h4>

<p>스파크는 CSV를 포함한 모든 종류의 파일을 읽을 수 있는 매우 편리한 방법을 제공합니다.</p>

<p>로컬 파일 시스템에있는 파일을 읽기 위해 해당 파일의 경로를 “spark.read.csv”를 이용합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prices</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">csv</span><span class="p">(</span><span class="s">"mnt/data_lake/landing/prices.csv"</span><span class="p">)</span>
<span class="n">prices</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>이것은 전체 파일 읽기를 즉시 트리거하지 않습니다. 그러나 일부 메타 데이터와 몇 바이트에 불과하므로 작업을 매우 빠릅니다.</p>

<p>이 DataFrame에 대한 작업을 수행 할 때까지 실제 실행이 연기됩니다.</p>

<p><code class="language-plaintext highlighter-rouge">show()</code>가 행동을 하게 합니다.</p>

<p>데이터 세트의 열을 볼 수 있지만, 헤더는 데이터의 일부로 저장됩니다.</p>

<p><br /></p>

<h4 id="reading-a-csv-file-with-headers">Reading a CSV file with headers</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prices</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">options</span><span class="p">(</span><span class="n">header</span><span class="o">=</span><span class="s">"true"</span><span class="p">).</span><span class="n">csv</span><span class="p">(</span><span class="s">"mnt/data_lake/landing/prices.csv"</span><span class="p">)</span>
<span class="n">prices</span><span class="p">.</span><span class="n">csv</span><span class="p">()</span>
</code></pre></div></div>

<p>위와 같이, header=”true”를 인수로 전달하면, 헤더가 전달됩니다. ( 표면적으로 보기 좋아짐 )</p>

<p><br /></p>

<h4 id="automatically-inferred-data-types">Automatically inferred data types</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>
<span class="n">pprint</span><span class="p">(</span><span class="n">price</span><span class="p">.</span><span class="n">dtypes</span><span class="p">)</span>
</code></pre></div></div>

<p>데이터 타입을 확인해보면 해당 데이터 프레임은 전부 string인 것을 알 수 있는데, Spark가 스키마를 추론하도록 할 수 있지만, 프로덕션에서는 잘못 유추 된 데이터 유형에 의존하지 않으므로, 일반적으로 스키마를 직접 정의합니다.</p>

<p><br /></p>

<h4 id="enforcing-a-schema">Enforcing a schema</h4>

<p>독자의 스키마를 통해, 스키마를 수동으로 지정 할 수 있습니다.</p>

<p>매서드를 사용하고, StructType 을 전달하여, StructFields목록을 캡슐화 합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span><span class="n">StructField</span><span class="p">(</span><span class="s">"store"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="n">nullable</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
                    <span class="n">StructField</span><span class="p">(</span><span class="s">"countrycode"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="n">nullable</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
                    <span class="n">StructField</span><span class="p">(</span><span class="s">"brand"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="n">nullable</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
                    <span class="n">StructField</span><span class="p">(</span><span class="s">"price"</span><span class="p">,</span> <span class="n">FloatType</span><span class="p">(),</span> <span class="n">nullable</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
                    <span class="n">StructField</span><span class="p">(</span><span class="s">"currency"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="n">nullable</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                    <span class="n">StructField</span><span class="p">(</span><span class="s">"quantity"</span><span class="p">,</span> <span class="n">IntegerType</span><span class="p">(),</span> <span class="n">nullable</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                    <span class="n">StructField</span><span class="p">(</span><span class="s">"date"</span><span class="p">,</span> <span class="n">DateType</span><span class="p">(),</span> <span class="n">nullable</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
                    <span class="p">])</span>
<span class="n">prices</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">options</span><span class="p">(</span><span class="n">header</span><span class="o">=</span><span class="s">"true"</span><span class="p">).</span><span class="n">schema</span><span class="p">(</span><span class="n">schema</span><span class="p">).</span><span class="n">csv</span><span class="p">(</span><span class="s">"mnt/data_lake/landing/prices.csv"</span><span class="p">)</span>
</code></pre></div></div>

<p>각 StructField는 CSV 파일의 한 열, 즉 이름, 유형 및 NULL 일 수 있는지 여부를 설명</p>

<p>이렇게 설정하면, 나중에 변환에 유리해집니다.</p>

<p><br /></p>

<h4 id="reasons-to-clean-data">Reasons to clean data</h4>

<p>데이터 과학자는 데이터 정제에 많은 시간을 사용하는데, 작업하는 대부분의 데이터 소스는 분석 할 준비가 되어 있지 않기 떄문입니다.</p>

<ul>
  <li>잘못된 유형의 데이터 ( csv에서 읽을 때, 모든 것이 문자열이기에 자주 발생 )</li>
  <li>잘못된 행 ( 특히 소스에서, 수동으로 입력 한 데이터를 구문 분석 할 때 엑셀 같은 일부 행에는 단순히 가짜 정보가 포함 )</li>
  <li>행이 불완전 ( 때로는 하나 또는 두 개를 제외하고 행이 거의 모든 필드가 유효합니다. )</li>
  <li>잘못 선택된 자리 표시 ( N/A 혹은 Unknown 등과 같은 문자열이 발생 - Null로 대체 )</li>
</ul>

<p><br /></p>

<h4 id="can-we-automate-data-cleanong">Can we automate data cleanong?</h4>

<p>데이터 과학자가 데이터를 정리하는데 너무 많은 시간을 소비한다면, 자동화하면 되지만, 자동화 할 수 없는 이유는, 데이터 정리는 종종 콘텍스트에 따라 달라지기 때문입니다.</p>

<ul>
  <li>모든 데이터 행이 중요한 규제 또는 엄격한 보고 환경에 있는가?</li>
  <li>다운 스트림 시스템이 95% 깨끗한 데이터와 95% 완전한 데이터를 처리 할 수 있는가?</li>
  <li>암시적 표준은 무엇인가? ( ex] 서유럽을 기반으로 하는 경우, 모든 날짜 시간을 중부 유럽 시간으로 설정 )</li>
  <li>데이터를 읽는 시스템의 하위 수준 세부 정보는 무엇인가?</li>
</ul>

<p><br /></p>

<h4 id="selecting-data-types">Selecting data types</h4>

<table>
  <thead>
    <tr>
      <th>Data type</th>
      <th>Value type in Python</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ByteType</td>
      <td>Good for numbers that -128 ~ 127</td>
    </tr>
    <tr>
      <td>ShortType</td>
      <td>Good for numbers that -32768 ~ 32767</td>
    </tr>
    <tr>
      <td>IntegerType</td>
      <td>Good for numbers that -2147483648 ~ 2147483647</td>
    </tr>
    <tr>
      <td>FloatType</td>
      <td>float</td>
    </tr>
    <tr>
      <td>StringType</td>
      <td>string</td>
    </tr>
    <tr>
      <td>BooleanType</td>
      <td>bool</td>
    </tr>
    <tr>
      <td>DateType</td>
      <td>datetime.date</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h4 id="badly-formatted-source-data">Badly formatted source data</h4>

<p>잘못 된 행이 있을경우, 스파크는 잘못된 행을 통합하기 위해 노력합니다. (null 처리)</p>

<p>하지만 이는 우리가 원하는 방법이 아닙니다.</p>

<p><br /></p>

<h4 id="handle-invalid-rows">Handle invalid rows</h4>

<p>스파크는 잘못된 행을 삭제하는 옵션을 제공합니다.</p>

<p>이를 위해 옵션 메소드 <code class="language-plaintext highlighter-rouge">mode</code>키워드에 <code class="language-plaintext highlighter-rouge">DROPMALFORMED</code>를 전달합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prices</span> <span class="o">=</span> <span class="p">(</span><span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">options</span><span class="p">(</span><span class="n">header</span><span class="o">=</span><span class="s">"true"</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">"DROPMALFORMED"</span><span class="p">).</span><span class="n">csv</span><span class="p">(</span><span class="s">'landing/prices.csv'</span><span class="p">))</span>
</code></pre></div></div>

<p>이렇게 설정하면, 잘못된 행이 자동으로 삭제됩니다.</p>

<p><br /></p>

<h4 id="the-significance-of-null">The significance of null</h4>

<p>잘못된 행이아닌, 불완전한 행일 경우, 해당 데이터를 모두 삭제하는 것은 이상적이지 않습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prices</span> <span class="o">=</span> <span class="p">(</span><span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">options</span><span class="p">(</span><span class="n">header</span><span class="o">=</span><span class="s">"true"</span><span class="p">).</span><span class="n">schema</span><span class="p">(</span><span class="n">schema</span><span class="p">).</span><span class="n">csv</span><span class="p">(</span><span class="s">'/landing/prices_with_incomplete_rows.csv'</span><span class="p">))</span>
<span class="n">prices</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Spark의 가장 기본적인 방법은 null값으로 공백을 채우는 방법입니다.</p>

<p><br /></p>

<h4 id="supplying-default-values-for-missing-data">Supplying default values for missing data</h4>

<p>누락된 데이터를 특정 값으로 채우도록 Spark에 지시 할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prices</span><span class="p">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s">'quantity'</span><span class="p">]).</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>이를 수행하는방법은 fillna 메소드를 사용하는 방법입니다. ( subset설정으로, 해당 열에만 영향 )</p>

<p><br /></p>

<h4 id="badly-chosen-placeholders">Badly chosen placeholders</h4>

<p>사람들은 종종 가치를 모르는 필드에 자리 표시자(placeholder)를 지정합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">employees</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">options</span><span class="p">(</span><span class="n">header</span><span class="o">=</span><span class="s">"true"</span><span class="p">).</span><span class="n">schema</span><span class="p">(</span><span class="n">schema</span><span class="p">).</span><span class="n">csv</span><span class="p">(</span><span class="s">"employees.csv"</span><span class="p">)</span>
</code></pre></div></div>

<p>이러한 자리 표시자는 분석 목적에 적합하지 않습니다. 대부분의 경우 알 수 없는 데이터를 단순히 null값 으로 처리하는 것이 좋습니다.</p>

<p><br /></p>

<h4 id="conditionally-replace-values">Conditionally replace values</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">col</span><span class="p">,</span> <span class="n">when</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">date</span><span class="p">,</span> <span class="n">timedelta</span>
<span class="n">one_year_from_now</span> <span class="o">=</span> <span class="n">date</span><span class="p">.</span><span class="n">today</span><span class="p">().</span><span class="n">replace</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">date</span><span class="p">.</span><span class="n">today</span><span class="p">().</span><span class="n">year</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">better_frame</span> <span class="o">=</span> <span class="n">employees</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"end_date"</span><span class="p">,</span> <span class="n">when</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">"end_date"</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">one_year_from_now</span><span class="p">,</span> <span class="bp">None</span><span class="p">)).</span><span class="n">otherwise</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">"end_date"</span><span class="p">))</span>
<span class="n">better_frame</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>여기서는 when() 함수의 조건을 사용하여 비논리적인 값을 대체합니다.</p>

<p>조건이 충족되면 값을 Python의 None으로 바뀝니다. 이 값은 Spark에서 null로 변환됩니다.</p>

<p><br /></p>

<h4 id="why-do-we-need-to-transform-data">Why do we need to transform data?</h4>

<p>통찰력은 일반적으로 데이터가 어느 정도 처리 된 후에 도출됩니다.</p>

<p>ex) 이웃에서 최고 평점을 받는 상위 10개의 호텔</p>

<p>정규화 과정, 위치 필터링 및 데이터 병합이 필요 할 수 있습니다. (훈련 된 기계 학습 모델을 적용하는 것도 데이터를 변환하는 것의 과정)</p>

<p><br /></p>

<h4 id="common-data-transformations">Common data transformations</h4>

<p>비지니스 로직 적용과 관련된 5가지 매우 일반적인 데이터 변환은 다음과 같습니다.</p>

<ul>
  <li>데이터 필터링 ( 특정 분석에 유용한 데이터에만 집중 )</li>
  <li>열을 선택 및 이름 변경 ( 해당 필드에 집중 할 수 있음 )</li>
  <li>
    <p>특정 필드별로 행을 그룹화 한 다음, 평균가격 및 샘플 수와 같읕 일부 메트릭 집계</p>
  </li>
  <li>특정 필드를 통해 연결하여, DataFrame를 결합하면, 데이터의 레코드에 더 많은 속성 부여 가능</li>
  <li>마지막으로 주문 데이터를 통해 우선 순위를 지정</li>
</ul>

<p><br /></p>

<h4 id="filtering-and-ordering-rows">Filtering and ordering rows</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prices_in_belgium</span> <span class="o">=</span> <span class="n">prices</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">'countrycode'</span><span class="p">)</span> <span class="o">==</span> <span class="s">'BE'</span><span class="p">).</span><span class="n">orderBy</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">'date'</span><span class="p">))</span>
</code></pre></div></div>

<p>부울 값이 있는 열을 “filter” 메소드에 전달하여 데이터를 필터링 할 수 있습니다.</p>

<p>ex) 이 예에서는 국가 코드가 문자열 “BE”와 같은 행만 유지됩니다.</p>

<ul>
  <li>col 함수를 사용하여, Columns 개체를 만들 수 있습니다.</li>
  <li>orderBy를 통해 정렬을 할 수 있습니다.</li>
</ul>

<p><br /></p>

<h4 id="selecting-and-renaming-columns">Selecting and renaming columns</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prices</span><span class="p">.</span><span class="n">select</span><span class="p">(</span>
	<span class="n">col</span><span class="p">(</span><span class="s">"store"</span><span class="p">),</span>
  <span class="n">col</span><span class="p">(</span><span class="s">"brand"</span><span class="p">).</span><span class="n">alias</span><span class="p">(</span><span class="s">"brandname"</span><span class="p">)</span>
<span class="p">).</span><span class="n">distinct</span><span class="p">()</span>
</code></pre></div></div>

<p>데이터가 있는 매장에서 판매 된 모든 브랜드를 알고 싶다고 가정하면, 선택 방법을 사용할 수 있습니다.</p>

<p>열의 이름을 바꾸려면 해당 열에서 “alias”메서드를 호출하기 만하면됩니다.</p>

<p>스파크에서는, DataFrame에서 distinct함수를 호출하여 고유 한 행만 반환 할 수 있습니다.</p>

<p><br /></p>

<h4 id="grouping-and-aggregating-with-mean">Grouping and aggregating with mean()</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">price</span><span class="p">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">'brand'</span><span class="p">)).</span><span class="n">mean</span><span class="p">(</span><span class="s">'price'</span><span class="p">)).</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>브랜드 별 평균 상품 가격을 알고 싶다면, 브랜드 열별로, 데이터를 그룹화하고, 각 그룹 내 가격의 평균을 호출 하는 것만큼 간단합니다.</p>

<p><br /></p>

<h4 id="grouping-and-aggregating-with-agg">Grouping and aggregating with agg()</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">prices</span><span class="p">.</span><span class="n">groupBy</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s">'brand'</span><span class="p">))</span>
				<span class="p">.</span><span class="n">agg</span><span class="p">(</span>
        		<span class="n">avg</span><span class="p">(</span><span class="s">"price"</span><span class="p">).</span><span class="n">alias</span><span class="p">(</span><span class="s">"average_price"</span><span class="p">),</span>
        		<span class="n">count</span><span class="p">(</span><span class="s">"brand"</span><span class="p">).</span><span class="n">alias</span><span class="p">(</span><span class="s">"number_of_items"</span><span class="p">))</span>
<span class="p">)</span>
</code></pre></div></div>

<p>평균과 갯수 모두를 원한단면, PySpark에서 agg메서드를 사용하면 됩니다.</p>

<p><br /></p>

<h4 id="joining-related-data">Joining related data</h4>

<p>일반적으로 조인을 통해 서로 다른 데이터 원본을 통합합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ratings_with_prices</span> <span class="o">=</span> <span class="n">ratings</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">prices</span><span class="p">,</span> <span class="p">[</span><span class="s">"brand"</span><span class="p">,</span> <span class="s">"model"</span><span class="p">])</span>
</code></pre></div></div>

<p>모든 행은 추가 정보로 보강됩니다.</p>

<p><br /></p>

<h4 id="running-your-pipeline-locally">Running your pipeline locally</h4>

<p>파이썬을 실행하는 것처럼, PySpark 데이터 파이프 라인을 로컬에서 실행 할 수 있습니다.</p>

<p>즉, 파이썬 인터프리터를 호출하고 기본 어플리케이션에 대한 경로를 전달합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">python</span> <span class="n">hello_world</span><span class="p">.</span><span class="n">py</span>
<span class="n">python</span> <span class="n">my_pyspark_data_pipeline</span><span class="p">.</span><span class="n">py</span>
</code></pre></div></div>

<p>스파크를 로컬에 설치한 경우 작동합니다.</p>

<p>파이프 라인에 필요한 모든 리소스와 올바르게 구성된 클래스 경로</p>

<p>클래스 경로는 Java에 알려준다는 점에서 PYTHONPATH와 유사한 역할을 합니다.</p>

<p><br /></p>

<h4 id="using-the-spark-submit-helper-program">Using the “spark-submit” helper program</h4>

<p>일상적인 작업에서는 <code class="language-plaintext highlighter-rouge">spark-submit</code> 스크립트를 사용하게 됩니다.</p>

<p>이 스크립트는 실행 환경을 설정하는 데 도움이 됩니다. 클러스터 관리자 및 선택한 배포 모드에 적합합니다. 클러스터 관리자는 다른 프로그램에서 사용할 수 있는 다른 노드의 RAM 및 CPU와 같은 클러스터 리소스 등을 관리합니다. ( YARN은 인기가 있으며, 각각 다른 옵션이 있습니다. )</p>

<p>이것이 <code class="language-plaintext highlighter-rouge">spark-submit</code>이 호출 매개 변수를 어느 정도 통합하려고 하는 이유입니다.</p>

<p>배포 모드는 Spark에 Spark 드라이버를 실행할 위치를 알려줍니다.</p>

<p>애플리케이션 : 전용 마스터 또는 클러스터 작업자 노드 중 하나</p>

<p>각 모드에는 장단점이 있으므로, 기술적으로 매우 빠릅니다.</p>

<p>시작 환경을 설정 한 후  “spark-submit”은 기본 클래스 또는 기본 메서드도 호출합니다.</p>

<p><br /></p>

<h4 id="basic-arguments-of-spark-submit">Basic arguments of “spark-submit”</h4>

<p>많은 PySpark애플리케이션의 경우 다음 템플릿 작업을 시작하는데 충분합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">spark</span><span class="o">-</span><span class="n">submit</span> \
	<span class="o">--</span><span class="n">master</span> <span class="s">"local[*]"</span> \
  <span class="o">--</span><span class="n">py</span><span class="o">-</span><span class="n">files</span> <span class="n">PY_FILES</span> \
  <span class="n">MAIN_PYTHON_FILE</span> \
  <span class="n">app_arguments</span>
</code></pre></div></div>

<p>Spark-submit을 호출합니다.</p>

<p>선택적 마스터 인수를 전달합니다. ( 이는 클러스터 관리자에 고유 URL이거나, local[*]과 같은 문자열입니다. 이는 실제 클러스터 또는 단순히 로컬 머신 리소스를 가져올 수 있는 위치를 Spark에게 알려줍니다.)</p>

<p>PySpark 데이터 파이프 라인이 하나 이상의 Python 모듈에 의존하는 경우 이 모듈을 모든 노드에 복사하여 각 Python 인터프리터가 클러스터 작업자는 실행해야하는 기능의 세부 정보를 어디서 찾을 수 있는지 알고 있습니다. 일반적으로, 작업자 노드에 아직 설치되지 않은 모듈은 zip파일을 통해 제공합니다.</p>

<p>그리고 이것이 <code class="language-plaintext highlighter-rouge">--py--files</code>인수가 우리에게 하는 일입니다.</p>

<p>마지막으로 Spark에 어떤 파일이 메인 파일인지, 어플리케이션의 진입 점인지 알려줍니다.</p>

<p>SparkSession 생성을 트리거하는 코드가 포함 된 파일입니다. 해당 파일인 경우 해당 Python 모듈이 실제로 “argparse”를 통해 명령 줄 인수를 구문 분석합니다. (예를 들어, 모듈에서 명령 끝에 이러한(app_arguments) 추가 인수를 제공 할 수 있습니다.)</p>

<p><br /></p>

<h4 id="collecting-all-dependencies-in-one-archive">Collecting all dependencies in one archive</h4>

<p><code class="language-plaintext highlighter-rouge">--py--files</code>옵션은 각각에 추가 될 쉼표로 구분 된 파일 목록을 사용할 수 있습니다.</p>

<p>Python 인터프리터가 모듈을 찾을 위치를 나열하는 작업자의 PYTHONPATH</p>

<p>Zip 파일은 코드를 패키징하고 배포하는 일반적인 방법입니다.</p>

<p>PySpark에서 사용할 준비가 된 적절한 zip파일을 만들려면, 모듈의 루트 폴더를 포함하는 디렉토리에 대한 쉘이 필요합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">zip</span> \
	<span class="o">--</span><span class="n">recurse</span><span class="o">-</span><span class="n">paths</span> \ 
  <span class="n">dependencies</span><span class="p">.</span><span class="nb">zip</span> \
  <span class="n">pydiaper</span>
  
  
<span class="n">spark</span><span class="o">-</span><span class="n">submit</span> \
	<span class="o">--</span><span class="n">py</span><span class="o">-</span><span class="n">files</span> <span class="n">dependencies</span><span class="p">.</span><span class="nb">zip</span> \ 
  <span class="n">pydiaper</span><span class="o">/</span><span class="n">cleaning</span><span class="o">/</span><span class="n">clean_prices</span><span class="p">.</span><span class="n">py</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">--recurse-paths</code>플래그를 전달하여 모든 하위 폴더의 모든 파일을 추가합니다.</p>

<p>결과 압축 아카이브의 이름을 제공하고, 마지막으로 압축하려는 폴더의 이름을 제공합니다.</p>

<p>결과 zip 파일은 “spark-submit”의 <code class="language-plaintext highlighter-rouge">--py-files</code>인수에 그대로 전달할 수 있습니다.</p>

<p><br /></p>

<h2 id="3-testing-your-data-pipeline">3. Testing your data pipeline</h2>

<p>웹 브라우저 같은 소프트웨어는 아직 완성되지 않았기 때문에, 제품이 변경되는 것은 당연합니다. 또한, 잘못된 계산이나 응답 불량과 같은 문제가 발견되고 해결되어야 합니다.</p>

<p>그러나 제품의 주요 측면도 일정하게 유지되어야 합니다.</p>

<p>근무 시간을 기준으로 사람들의 임금을 계산하는 코드는 지속적이고 잘 테스트 되는 것이 좋습니다.</p>

<p><br /></p>

<h4 id="rationale-behind-testing">Rationale behind testing</h4>

<p>우리의 기대치가 일치 함을 확인하기 위해 테스트가 작성되고 실행됩니다. 코드 기반에 대한 테스트를 수행함으로써, 우리는 과거의 어느 시점에 그랬던 것처럼, 우리의 기대에 대한 서면 사본을 갖게됩니다.</p>

<p>따라서 테스트를 통해 주요 변경 사항을 도입 할 가능성이 줄어듭니다.</p>

<p>테스트가 없으면, 함수가 호출될 때까지 결과를 알 수 없습니다. ( 비용적인 측면도 많이 듬 )</p>

<p>자동화된 테스트는 이를 방지하는 데 도움을 줍니다. 테스트는 거의 완전하지 않습니다. ( 모든 경우의 수를 다해 보는것이 아니라, 몇개의 샘플만 테스트하도록 선택합니다. - 따라서 테스트가 모든 경우에 대한 정확성을 보장하지는 않습니다. )</p>

<p>테스트의 필요성에 대한 다른 주장은, 테스트가 가장 최신의 문서형식이라는 것입니다.</p>

<p>종종 사람들은 일부 소프트웨어의 기능을 수동으로 설명하는 문서를 작성합니다.</p>

<p>설정이 높은 수준으로 유지되는 한 괜찮습니다. 특정 기능에 대한 세부사항은 피해야합니다. ( 실제로 실행되는 것과, 동기화되지 않는 경향이 있습니다. )</p>

<p>잘 작성된 테스트는 코드가 블랙 박스로 취급 되더라도 코드가 무엇을하는지 명확히 할 수 있습니다.</p>

<p>코드를 테스트해야하는 이유가 더 많지만, 이 세가지 이유로 충분한 인센티브를 제공할 수 있습니다.</p>

<p><br /></p>

<h4 id="the-test-pyramid-where-to-invest-your-efforts">The test pyramid: where to invest your efforts</h4>

<p>때때로 사람들은 테스트를 피하는데, 소프트웨어에 더 많은 기능을 추가하는데 더 많은 시간을 소요합니다.</p>

<p>테스트에 시간이 걸리는 것은 사실이지만, 프로젝트로 보답하는 것은 투자입니다.</p>

<p>특히 테스트가 다른 위치에서 복제되지 않고, 중요하지 않은 부분을 테스트 할 때 더 커집니다.</p>

<p>테스트를 설계 할 때 명심해야 할 유용한 개념은 테스트 피라미드입니다.</p>

<p>노력을 어디에 투자해야하는지 알려줍니다.</p>

<p>외부 구성 요소와의 통합에 의존하지 않는 코드 조각을 테스트하는 것은 단위 테스트입니다. 이들은 빠르게 실행될 수 있고 개발 노력이 저렴합니다.</p>

<p>이러한 테스트는 매우 빠르게 실행되기 때문에 많은 테스트가 있어야 합니다.</p>

<p>파일 시스템 및 데이터베이스와의 상호 작용은 통합 테스트입니다.</p>

<p>통합 테스트(서비스 테스트)는 서로 다른 서비스 또는 구성요소를 통합합니다. 그들은 더 느리게 실행되고, 설정에도 더 많은 노력이 필요합니다. 단위 테스트가 이미 일부를 다루어야 하기 때문에 적은 수를 가질 수 있습니다.</p>

<p>엔드 투 엔드 테스트에서도 마찬가지입니다. 사용자 인터페이스 (UI)를 통해 상호작용할 때 사용자가 얻을 수 있는 경험 느리게 실행되고 자주 발생하므로 비용이 훨씬 더 많이 듭니다. 강력하게 작성하기는 어렵지만, 최종 사용자의 경험에 가깝습니다.</p>

<p><br /></p>

<h4 id="spark-application-is-an-etl-pipeline">Spark application is an ETL pipeline</h4>

<ul>
  <li>위치에서 추출 된 데이터</li>
  <li>변형</li>
  <li>다른 위치에 로드</li>
</ul>

<p>첫 번째 단계(추출)와 마지막 단계(저장) 단계에서는, 데이터베이스, 파일 시스템 및 클라우드 제공 업체와 같은 다른 서비스와 상호 작용합니다.</p>

<p>이러한 상호 작용은, 비용이 많이 들고, 이전에서 알 수 있듯이, 자주 일어나면 안됩니다. 이것이 우리가 진정한 비지니스 가치를 추가하는 곳입니다.</p>

<p><br /></p>

<h4 id="separte-transform-from-extract-and-load">Separte transform from extract and load</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">prices_with_ratings</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"..."</span><span class="p">)</span> <span class="c1"># extract
</span><span class="n">exchange_rates</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">csv</span><span class="p">(</span><span class="s">"..."</span><span class="p">)</span> <span class="c1"># extract
</span>
<span class="n">unit_prices_with_ratings</span> <span class="o">=</span> <span class="p">(</span><span class="n">prices_with_ratings</span><span class="p">.</span><span class="n">join</span><span class="p">(...).</span><span class="n">withColumn</span><span class="p">(...))</span> <span class="c1"># transform
</span></code></pre></div></div>

<p>변환은 파일 시스템과의 상호 작용을 통해 얻은 DataFrames에서 작동합니다. ( 의존성을 제거하고 전적으로 변화에만 집중하길 원함 )</p>

<p><br /></p>

<h4 id="construct-dataframes-in-memory">Construct DataFrames in-memory</h4>

<p>( 변환이 더 이상 파일에서 데이터를 가져 오지 않는 경우에도 데이터를 어떻게 공급하는가? )</p>

<p>Spark DataFrames는 메모리 내에서도 생성 될 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Extract the data
</span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">csv</span><span class="p">(</span><span class="n">path_to_file</span><span class="p">)</span>
</code></pre></div></div>

<p>인메모리를 생성하여 해결하는 것은 여러가지 단점이 있습니다.</p>

<ul>
  <li>input/output에 의존하게 됨</li>
  <li>unclear how big the data is</li>
  <li>unclear what data goes in</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="n">purchase</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="s">"price"</span><span class="p">,</span> <span class="s">"quantity"</span><span class="p">,</span> <span class="s">"product"</span><span class="p">)</span>
<span class="n">record</span> <span class="o">=</span> <span class="n">purchase</span><span class="p">(</span><span class="mf">12.99</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s">"cake"</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
</code></pre></div></div>

<p>사용자 지정 Spark SQL Row 클래스를 만들고 이 클래스 인스턴스의 반복 가능한 모든 인스턴스를 createDataFrame() 생성자에 전달합니다.</p>

<p>여기에는 분명한 이점이 있습니다.</p>

<ul>
  <li>데이터가 함수로 전달 되는 것을 즉시 확인</li>
  <li>파일을 찾거나 다운로드하기 위해 여기저기서 클릭 할 필요가 없습니다.</li>
</ul>

<p><br /></p>

<h4 id="create-small-reusable-and-well-named-functions">Create small, reusable and well-named functions</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">link_with_exchange_rates</span><span class="p">(</span><span class="n">prices</span><span class="p">,</span> <span class="n">rates</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">prices</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">rates</span><span class="p">,</span> <span class="p">[</span><span class="s">"currency"</span><span class="p">,</span> <span class="s">"date"</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">calculate_unit_price_in_euro</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">df</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"unit_price_in_euro"</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s">"price"</span><span class="p">)</span><span class="o">/</span><span class="n">col</span><span class="p">(</span><span class="s">"quantity"</span><span class="p">)</span><span class="o">*</span><span class="n">col</span><span class="p">(</span><span class="s">"exchange_rate_to_euro"</span><span class="p">))</span>

<span class="n">unit_prices_with_ratings</span> <span class="o">=</span> <span class="p">(</span> 
  <span class="n">calculate_unit_price_in_euro</span><span class="p">(</span>
		<span class="n">link_with_exchange_rates</span><span class="p">(</span><span class="n">price</span><span class="p">,</span> <span class="n">exchange_Rates</span><span class="p">)</span>
  <span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<p>이 전에는 코드를 하나에 여러 일을 작동하게 했지만, 그렇게하면 코드를 테스트 하기 힘듭니다. 따라서 다음과 같이, 각 단계를 자체 기능에 기록합니다.</p>

<p>새로운 함수를 작성하는 것은 어리석은 것 처럼 보일 수 있지만, 단일 변환, 각 변환 자체를 테스트 할 수 있습니다. 또한 재사용 될 수 있습니다.</p>

<p><br /></p>

<h4 id="testing-a-single-unit">Testing a single unit</h4>

<p>단가를 유로로 계산하는 테스트를 작성하는 코드</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test_calculate_unit_price_in_euro</span><span class="p">():</span>
  <span class="n">record</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">price</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
               <span class="n">quantity</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
               <span class="n">exchange_rate_to_euro</span><span class="o">=</span><span class="mf">2.</span><span class="p">)</span>
  <span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="o">**</span><span class="n">record</span><span class="p">)])</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">calculate_unit_price_in_euro</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
  
  <span class="n">expected_record</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="o">**</span><span class="n">record</span><span class="p">,</span> <span class="n">unit_price_in_euro</span><span class="o">=</span><span class="mf">4.</span><span class="p">)</span>
  <span class="n">expected</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">expected_record</span><span class="p">])</span>
  
  <span class="n">assertDataFrameEqual</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">expected</span><span class="p">)</span>
</code></pre></div></div>

<p>필요한 열만 있는 메모리 내 DataFrame을 만드는 것으로 시작합니다.</p>

<p>Row에 record딕셔너리를 압축하는 방법으로, 언패키징 방법을 사용합니다.</p>

<p>DataFrame을 테스트하려는 함수에 전달하고 값을 비교합니다. (이 방법은 직접 쉽게 계산할 수 있으며, 메모리 내 DataFrame으로 테스트를 실행하는 또 다른 이점입니다. )</p>

<p>이제 시작 사전을 사용한 이유도 알 수 있는데, 초기 및 예상 DataFrame 모두에서 반복됩니다.</p>

<p><br /></p>

<h4 id="take-home-messages--함수화-하면-좋은-점">Take home messages ( 함수화 하면 좋은 점)</h4>

<ul>
  <li>외부 데이터 소스와 상호 작용하는 것은 값 비싼 메모리 내 DataFrame를 사용하면 단일 기능을 보다 쉽게 테스트 할 수 있습니다.</li>
  <li>눈에 잘 띄는 데이터와 소수의 예에 초점을 맞출수 있음</li>
  <li>당신의 코드는 리팩토링되어 작게 재사용 가능하며, 이름이 잘 지정된 함수는 테스트 하기 더 쉽습니다.</li>
</ul>

<p><br /></p>

<h4 id="running-a-test-suite">Running a test suite</h4>

<p>많은 프로그래밍 언어가 테스트를 실행하는 기능을 제공합니다. 파이썬에도 이를 수행하는 여러 모듈이 있습니다.</p>

<ul>
  <li>unittest</li>
  <li>doctest</li>
  <li>pytest</li>
  <li>nose</li>
</ul>

<p>모든 경우에 이러한 도구는 모듈, 클래스 및 함수를 찾습니다. ( 접두사(test 등) 을 통해 드릴 다운하여 실행할 수 있는 코드를 찾은 다음 실행 )</p>

<p>그들의 핵심 임무는 무언가를 주장하는 것입니다. ( 계산 된 값이, 예상 값과 같거나 오류가 발생했습니다. )</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">assert</span> <span class="n">computed</span> <span class="o">==</span> <span class="n">expected</span>
<span class="k">with</span> <span class="n">pytest</span><span class="p">.</span><span class="n">raises</span><span class="p">(</span><span class="nb">ValueError</span><span class="p">)</span>
</code></pre></div></div>

<p><br /></p>

<h4 id="manually-triggering-test">Manually triggering test</h4>

<p>이러한 프레임 워크의 가장 큰 특징은 실행이 끝날 때 보고서를 생성한다는 것입니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cd</span> <span class="o">~/</span><span class="n">workspace</span><span class="o">/</span><span class="n">my_good_python_project</span>
<span class="n">pytest</span> <span class="p">.</span>
</code></pre></div></div>

<p>어떤 테스트가 통과 되었는지, 어떤 테스트가 실패하였는지 요약하고, 종종 디버깅에 도움이 되는 추가 정보를 제공합니다.</p>

<p>이전에 단위 테스트가 빠르게 실행되어야 한다고 말했는데, 단일 테스트늬 경우 2초는 긴 시간입니다.</p>

<p>Spark 및 기타 분산 컴퓨팅 프레임 워크는 테스트에 오버 헤드를 추가합니다.</p>

<p><br /></p>

<h4 id="automating-tests">Automating tests</h4>

<p>자동화는 데이터 엔지니어의 주요 목표 중 하나입니다.</p>

<p>단위 테스트를 수동으로 실행하는 것은, 지루하고 오류가 발생하기 쉽습니다.</p>

<p>일련의 코드 변경 후 실행하는 것을 잊을 수 도 있습니다.</p>

<p>코드를 전문적으로 작업하는 경우, git 과 같은 일종의 버전 제어 시스템을 사용하는 것입니다.</p>

<p>많은 버전 제어 시스템에서 코드를 변경 할 때, 실행할 특정 스크립트를 구성 할 수 있습니다.</p>

<p>변경 사항에 대한 빠른 피드백을 제공할 수 있으므로 훌륭합니다.</p>

<p>버전 제어 시스템에서 제공하는 옵션을 탐색하는 것이 좋습니다.</p>

<p><br /></p>

<h4 id="ci--cd">CI / CD</h4>

<p>두 번째 방어선은 프로젝트의 CI/CD 파이프라인입니다.</p>

<p>CI / CD는 Continuous Integration 및 Continuous Delivery를 나타냅니다.</p>

<p><br /></p>

<p>Continuous Integration</p>

<ul>
  <li>마스터 브랜치라고 하는 프로덕션에서 실행되는 코드로 가능한 빨리 변경 사항으로 인해 테스트가 어느 정도 감지 할 수 있는 문제가 없는 경우에만 허용</li>
  <li>테스트를 실행하고 많은 것을 보유하는 데 집중합니다.</li>
</ul>

<p><br /></p>

<p>Continuous Delivery</p>

<ul>
  <li>모든 아티팩트가 문제없이 항상 가능한 상태에 있어야 합니다.</li>
  <li>일부 버전 제어 시스템의 일부로 코드를 일부 원격 서버에 푸시 할 때, Python 스타일 가이드 인 PEP8 준수와 같은 실행중인 단위 테스트 및 정적 코드 검사를 트리거 합니다.</li>
</ul>

<p><br /></p>

<h4 id="configuring-a-cicd-tool">Configuring a CI/CD tool</h4>

<ul>
  <li>CircleCI : 자동으로 테스트를 실행하는 서비스 ( 이러한 많은 도구와 마찬가지로 코드 저장소에서 특정 파일을 찾습니다. ) 코드 저장소 루트에 있어야하는 circleci 폴더의 config.yml 파일을 찾습니다. 구문은 JSON의 상위 집합인 YAML이지만, 초점이 다르며 많은 기능을 추가합니다.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">jobs</span><span class="p">:</span>
  <span class="n">test</span><span class="p">:</span>
    <span class="n">docker</span><span class="p">:</span>
      <span class="o">-</span> <span class="n">image</span><span class="p">:</span> <span class="n">circleci</span><span class="o">/</span><span class="n">python</span><span class="p">:</span><span class="mf">3.6</span><span class="p">.</span><span class="mi">4</span>
    <span class="n">steps</span><span class="p">:</span>
      <span class="o">-</span> <span class="n">checkout</span>
      <span class="o">-</span> <span class="n">run</span><span class="p">:</span> <span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">r</span> <span class="n">requirements</span><span class="p">.</span><span class="n">txt</span>
      <span class="o">-</span> <span class="n">run</span><span class="p">:</span> <span class="n">pytest</span> <span class="p">.</span>
</code></pre></div></div>

<p>구성 파일에는 작업이라는 세션이 있습니다. 각 작업에는 테스트와 같은 이름이 있습니다.</p>

<p>작업은 일부에서 실행되는 단계 모음입니다. 이 경우 Python의 특정 버전을 위해 설계된 도커 이미지입니다.</p>

<p>단계는 순서대로 실행되며, 일반적으로 새 프로젝트를 받은 경우 수동으로 실행할 단계</p>

<p>이 경우 CircleCI에 코드에서</p>

<ul>
  <li>코드를 확인하도록 지시합니다.</li>
  <li>필요한 모듈 설치 및 pytest로 테스트 스위트 실행</li>
  <li>테스트 실행</li>
  <li>테스트가 성공하면, 애플리케이션을 패키징하여 서버에 배포하거나 나중에 사용할 수 있도록 저장할 수 있습니다.</li>
</ul>

<p><br /></p>

<h4 id="4-managing-and-orchestrating-a-workflow">4. Managing and orchestrating a workflow</h4>

<p><br /></p>

<h4 id="what-is-a-workflow">What is a workflow?</h4>

<p>워크 플로는 예약 된 일련의 작업입니다.</p>

<p>실행하거나 이벤트 발생에 의해 트리거 될 수 있습니다.</p>

<p>워크 플로는 일반적으로 데이터 과학자와 데이터 엔지니어가 데이터 처리 파이프 라인을 조정하는 데 사용됩니다.</p>

<p>종종 일정에 따라 실행해야하는 작업이 있습니다.</p>

<p><br /></p>

<h4 id="scheduling-with-cron">Scheduling with cron</h4>

<p>예를 들어 변동 예측 알고리즘은 매주 실행해야 할 수 있습니다.</p>

<p>이러한 경우에 자주 사용되는 소프트웨어 유틸리티는 “cron”입니다.</p>

<p>Cron은 “crontab” 파일로 알려진 구성 파일을 읽습니다.</p>

<p>이러한 파일에서 특정 시간에 실행할 작업을 표로 만듭니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">*/</span><span class="mi">15</span> <span class="mi">9</span><span class="o">-</span><span class="mi">17</span> <span class="o">*</span> <span class="o">*</span> <span class="mi">1</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span> <span class="n">log_my_activity</span>
</code></pre></div></div>

<p>이 crontab 레코드는 특정 시간에 “log_my_activity”프로세스를 실행합니다.</p>

<p>15분마다 9-17시에  매일, 매월, 1(월), 2(화), 3(수), 5(금) 에 다음을  실행한다.</p>

<p>크론탭은 매우 기본적인 스케쥴러입니다.</p>

<p>매우 잘 작동하지만, 시간이 지남에 따라 모니터링하려는 복잡한 워크 플로에 가장 적합한 도구는 아닙니다.</p>

<ul>
  <li>Luigi</li>
  <li>Azkaban</li>
  <li>Airflow</li>
</ul>

<p>위와 같은 도구를 사용 하는 것이 더 효율적입니다.</p>

<p><br /></p>

<h4 id="apache-airflow-fulfills-modern-engineering-needs">Apache Airflow fulfills modern engineering needs</h4>

<p>최신 워크 플로 관리 도구에서 기대하는 것은 복잡한 워크 플로를 만들고 시각화 할 수 있다는 것입니다.</p>

<p>Airflow가 앞서 본 워크 플로 회로도를 시각화 하는 방법은 다음과 같습니다.</p>

<p align="center"><img src="/images/post_img/data_airflow.png" /></p>

<p>이러한 도구를 통해 워크 플로를 모니터링 할 수도 있습니다.</p>

<p>Airflow는 특정 작업이 실패한시키 또는 각 작업에 걸린 시간을 보여주고, 이를 명확한 차트로 표시할 수 있습니다.</p>

<p>마지막으로, 수평 확장이 중요합니다.</p>

<p>더 많은 작업을 실행할수록 도구가 단일 시스템의 성능을 높이는 대신, 여러 시스템을 사용합니다.</p>

<p>Airflow는 이러한 모든 요구 사항을 충족하므로, cron을 대체하는 것이 좋습니다.</p>

<p><br /></p>

<h4 id="the-directed-acyclic-graph-dag">The Directed Acyclic Graph (DAG)</h4>

<p>Airflow 워크 플로의 중심 부분은 Directed Acyclic Graph인 DAG입니다.</p>

<p>그래프는 간선으로 연결된 노드 모음입니다.</p>

<p align="center"><img src="/images/post_img/data_airflow2.png" /></p>

<p>두문자어의 지시 된 부분은, 노드 사이에 방향 감각이 있음을 의미합니다.</p>

<p>가장자리의 화살표는 방향을 나타냅니다.</p>

<p>“비순환” 부분은 단순히 횡단 일 떄, 방향 그래프를 사용하면 동일한 노드로 다시 동그라미를 칠 수 없습니다.</p>

<p>Airflow에서 노드는 “운영자” 이며, 각 인스턴스에는 고유 한 레이블 인 작업 ID가 부여될 수 있습니다.</p>

<p>운영자는 Python 스크립트를 실행하거나 클라우드 공급자를 통해 작업을 예약하는 등의 작업을 수행할 수 있습니다.</p>

<p>스케쥴러에 의해 트리거되지만, 일반적으로 다른 프로세스인 실행기에 의해 실행됩니다.</p>

<p><br /></p>

<h4 id="the-directed-acyclic-graph-in-code">The Directed Acyclic Graph in code</h4>

<p>코드에서 Airflow DAG에는 ‘dag_id’가 필요합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">airflow</span> <span class="kn">import</span> <span class="n">DAG</span>

<span class="n">my_dag</span> <span class="o">=</span> <span class="n">DAG</span><span class="p">(</span>
	<span class="n">dag_id</span><span class="o">=</span><span class="s">"publish_logs"</span><span class="p">,</span>
  <span class="n">schedule_interval</span><span class="o">=</span><span class="s">"* * * * *"</span><span class="p">,</span>
  <span class="n">start_date</span><span class="o">=</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2010</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div>

<p>가장 일반적인 선택적 인수 중 일부는 ‘schedule_interval’ 입니다.</p>

<p>cron 일정과 DAG가 시작되어야하는 날짜를 전달할 수 있습니다.</p>

<p>연산자는 나중에 코드에서 이 DAG에 할당됩니다.</p>

<p><br /></p>

<h4 id="classes-of-operators">Classes of operators</h4>

<p>작업은 기본적으로 연산자 클래스의 인스턴스입니다.</p>

<p>Airflows는 여러 가지와 함께 제공되지만 고유 한 연산자를 정의 할 수 도 있습니다.</p>

<p>일반적으로 수행하는 작업에 따라 이름이 저장됩니다.</p>

<ul>
  <li>BashOperator -&gt; bash 커맨드/스크립트</li>
  <li>PythonOperator -&gt; Python</li>
  <li>SparkSubmitOperator -&gt; 스파크</li>
</ul>

<p><br /></p>

<h4 id="expressing-dependencies-between-operators">Expressing dependencies between operators</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dag</span> <span class="o">=</span> <span class="n">DAG</span><span class="p">(...)</span>
<span class="n">task1</span> <span class="o">=</span> <span class="n">BashOperator</span><span class="p">(...)</span>
<span class="n">task2</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(...)</span>
<span class="n">task3</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(...)</span>

<span class="n">task1</span><span class="p">.</span><span class="n">set_downstream</span><span class="p">(</span><span class="n">task2</span><span class="p">)</span>
<span class="n">task3</span><span class="p">.</span><span class="n">set_upstream</span><span class="p">(</span><span class="n">task2</span><span class="p">)</span>

<span class="c1"># task1 &gt;&gt; task2
# task3 &lt;&lt; task2
# or
# task1 &gt;&gt; task2 &gt;&gt; task3
</span></code></pre></div></div>

<p>연산자 간의 종속성은 다음과 같이, “set_upstream()” 및 “set_downstream()” 메서드로 코딩됩니다.</p>

<p>다음과 같이 비트 시프트 연산자를 사용하여 이러한 동일한 관계를 정의 할 수 도 있습니다.</p>

<p>이와 같은 간단한 DAG 설명은 선형흐름을 생성합니다.</p>

<p>연산자의 인수를 사용하면 다음을 지정할 수 있습니다.</p>

<p><br /></p>

<h4 id="airflows-bashoperator">Airflow’s BashOperator</h4>

<p>Airflow의 BashOperator를 사용하여 bash 쉘에서도, 실행할 수 있는 모든 bash 명령을 실행합니다.</p>

<p>spark-submit과 Singer 수집 파이프 라인은 모두 이러한 방식으로 트리거 할 수 있는 명령입니다.</p>

<p>Airflow를 통해 이러한 명령을 실행하는 한 가지 이점은 실행이 기록되고 모니터링되며 재 시도된다는 것입니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">airflow.operators.bash_operator</span> <span class="kn">import</span> <span class="n">BashOperator</span>

<span class="n">bash_task</span> <span class="o">=</span> <span class="n">BashOperator</span><span class="p">(</span>
	<span class="n">task_id</span><span class="o">=</span><span class="s">'greet_world'</span><span class="p">,</span>
  <span class="n">dag</span><span class="o">=</span><span class="n">dag</span><span class="p">,</span>
  <span class="n">bash_command</span><span class="o">=</span><span class="s">'echo "Hello, world!"'</span>
<span class="p">)</span>
</code></pre></div></div>

<p>모든 Airflow 연산자는 사용자가 선택할 수 있는 고유 식별자인 “task_id”를 사용합니다.</p>

<p>이 작업을 할당하는 DAG 개체에 대한 참조는</p>

<p>모든 연산자는 조상 인 BashOperator 클래스에서 상속하기 때문입니다.</p>

<p>마지막으로 “bash_command”에서 BashOperator에서 실행할 bash 명령을 문자열로 지정합니다.</p>

<p>이 중 여러 개를 지정하거나, 이름으로 스크립트 파일을 지정할 수 도 있습니다.</p>

<p><br /></p>

<h4 id="airflows-pythonoperator">Airflow’s PythonOperator</h4>

<p>BashOperator를 사용하여 많은 작업을 수행 할 수 있지만, “python my_app”과 같은 적절한 bash 명령을 전달하여 모든 사례를 제공하지는 않습니다.</p>

<p>예를 들어 Python 모듈을 수정할 수는 없지만, 필요한 기능이있는 경우 BashOperator는 많이 사용되지 않습니다.</p>

<p>이 경우 “PythonOperator”를 사용하는 것이 훨씬 더 적절합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">airflow.operators.python_operator</span> <span class="kn">import</span> <span class="n">PythonOperator</span>
<span class="kn">from</span> <span class="nn">my_library</span> <span class="kn">import</span> <span class="n">my_magic_function</span>

<span class="n">python_task</span> <span class="o">=</span> <span class="n">PythonOperator</span><span class="p">(</span>
	<span class="n">dag</span><span class="o">=</span><span class="n">dag</span><span class="p">,</span>
	<span class="n">task_id</span><span class="o">=</span><span class="s">'perform_magic'</span><span class="p">,</span>
	<span class="n">python_callable</span><span class="o">=</span><span class="n">my_magic_function</span><span class="p">,</span>
  <span class="n">op_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s">"snowflake"</span><span class="p">:</span> <span class="s">"*"</span><span class="p">,</span> <span class="s">"amount"</span><span class="p">:</span> <span class="mi">42</span><span class="p">}</span>
<span class="p">)</span>
</code></pre></div></div>

<p>PythonOperator의 “python_callable”인수는 함수와 같이 호출 될 수 있는 모든 것 뿐만 아니라 클래스도 가능합니다.</p>

<p>여기에서 실제로 콜러블 이라고 부르지 않는 점에 주의해주세요.</p>

<p>작업이 트리커되면 운영자가 호출합니다.</p>

<p>아 콜러블에 추가 인수가 필요한 경우 선택적  키워드 인수 “op_args” 및 “op_kwargs”를 사용하여 전달할 수 있습니다.</p>

<p><br /></p>

<h4 id="running-pyspark-from-airflow">Running Pyspark from Airflow</h4>

<p>Airflow를 사용하여 이러한 파이프 라인을 호출하고 싶다면, 이를 수행하는 몇 가지 방법이 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">spark_master</span> <span class="o">=</span> <span class="p">(</span>
	<span class="s">"spark://"</span>
	<span class="s">"spark_standalone_cluster_ip"</span>
	<span class="s">":7077"</span><span class="p">)</span>
<span class="n">command</span> <span class="o">=</span> <span class="p">(</span>
	<span class="s">"spark-submit "</span>
	<span class="s">"--master {master}"</span>
	<span class="s">"--py-files package1.zip "</span>
	<span class="s">"/path/to/app.py"</span>
<span class="p">).</span><span class="nb">format</span><span class="p">(</span><span class="n">master</span><span class="o">=</span><span class="n">spark_master</span><span class="p">)</span>
<span class="n">BashOperator</span><span class="p">(</span><span class="n">bash_command</span><span class="o">=</span><span class="n">command</span><span class="p">,</span> <span class="p">...)</span>
</code></pre></div></div>

<p>BashOperator를 사용하여 “spark-submit”을 호출 할 수 있습니다.</p>

<p>명령을 문자열로 지정한다는 점을 기억해야합니다.</p>

<p>여기서는 가독성을 높이기 위해 괄호를 사용하여 명령의 인수를 분할했습니다.</p>

<p>Airflow 서버의 bash셀에서 명령을 성공적으로 실행할 수 있다면 작동합니다.</p>

<p>이 접근 방식의 단점은 Airflow 서버에 Spark 바이너리를 설치해야한다는 점입니다.</p>

<p>이를 수행하는 또다른 방법은 Airflow의 SSHOperator를 사용하여 다른 서버에 위임하는 것 입니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">airlfow.contrib.operators.ssh_operator</span> <span class="kn">import</span> <span class="n">SSHOperator</span>

<span class="n">task</span> <span class="o">=</span> <span class="n">SSHOperator</span><span class="p">(</span>
	<span class="n">task_id</span><span class="o">=</span><span class="s">'ssh_spark_submit'</span><span class="p">,</span>
	<span class="n">dag</span><span class="o">=</span><span class="n">dag</span><span class="p">,</span>
	<span class="n">command</span><span class="o">=</span><span class="n">command</span><span class="p">,</span>
	<span class="n">ssh_conn_id</span><span class="o">=</span><span class="s">'spark_master_ssh'</span>
<span class="p">)</span>
</code></pre></div></div>

<p>이 연산자는 모든 타사 기여 연산자를 포함하는 contrib 패키지에 속합니다.</p>

<p>BashOperator에 비해 이 연산자는 Spark바이너리를 다른 컴퓨터에 설치해야합니다.</p>

<p>Spark 지원 클러스터에 원격으로 액세스 할 수 있는 경우 이는 매우 깔끔하게 진행할 수 있는 방법입니다.</p>

<p>다시 한번 문자열 형식으로 실행할 명령을 추가합니다.</p>

<p>이 연산자의 “ssh_conn_id”인수를 확인하십시오.</p>

<p>관리 메뉴의 연결 아래에 있는 Airflw사용자 인터페이스에서 구성할 수 있는 연결을 나타냅니다.</p>

<p>이러한 연결은 다음과 같은 편리한 방법을 제공합니다.</p>

<p>다른 서버에 대한 연결 세부 정보와 관련된 하드 코디되고 중복된 정보를 제거합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">airflow.contrib.operators.spark_submit_operator</span> <span class="kn">import</span> <span class="n">SparkSubmitOperator</span>

<span class="n">spark_task</span> <span class="o">=</span> <span class="n">SparkSubmitOperator</span><span class="p">(</span>
	<span class="n">task_id</span><span class="o">=</span><span class="s">'spark_submit_id'</span><span class="p">,</span>
	<span class="n">dag</span><span class="o">=</span><span class="n">dag</span><span class="p">,</span>
	<span class="n">application</span><span class="o">-</span><span class="s">"/path/to/app.py"</span><span class="p">,</span>
	<span class="n">py_files</span><span class="o">=</span><span class="s">"package1.zip"</span><span class="p">,</span>
	<span class="n">conn_id</span><span class="o">=</span><span class="s">'spark_default'</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Spark 작업을 실행하는 또 다른 방법은 “SparkSubmitOperator”를 사용하는 것입니다.</p>

<p>이것도 기여한 운영자에게 속합니다.</p>

<p>Airflow 서버에서 직접 “spark-submit”을 사용하는 것이므로, BashOperator를 사용하는 솔루션과는 많이 다르지만, 키워드 인수를 제공합니다.</p>

<p>spark-submitd의 인수를 위해 전체 명령을 문자열로 작성할 필요가 없습니다.</p>

<p>또한 SSHOperator와 동일한 방식으로 연결을 사용합니다.</p>

<p>다른 작업에서 DAG는 모두 동일한 spark지원 클러스터에 연결되어 있으면 간단합니다.</p>

<p><br /></p>

<h4 id="installing-and-configuring-airflow">Installing and configuring Airflow</h4>

<p>워크 플로가 이제 Airflow DAG에 완전히 설명되 있지만 이를 실행할 준비가 된 서버를 만들지 않았습니다.</p>

<p>시작하는 가장 쉬운 방법은 Linux 이미지에 설치하는 것입니다.</p>

<p>Linux를 실행하는 로컬 서버를 사용할 수 있다고 가정합시다.</p>

<p>다른 어떤 것도 실행하지 않고 Python 환경이 깨끗합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">export</span> <span class="n">AIRFLOW_HOME</span><span class="o">=~/</span><span class="n">airflow</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">apache</span><span class="o">-</span><span class="n">airflow</span>
<span class="n">airflow</span> <span class="n">initdb</span>
</code></pre></div></div>

<p>셀에서 일부 디렉토리를 AIrflow의 홈 디렉토리로 선언합니다.</p>

<p>다음으로 Airflow를 설치합니다.</p>

<p>이후, 메타 데이터 데이터베이스를 초기화합니다.</p>

<p>이와 같이 새로 설치하면 AIRFLOW_HOME 디렉퇴가 생성되고 모든 로그 파일, 두 개의 구성 파일 및 SQLite 데이터베이스에 대한 하위 디렉토리로 채웁니다.</p>

<p>프로덕션에서는 MySQL과 같은 고급 데이터베이스를 사용할 가능성이 높습니다.</p>

<p>SequentialExecutor를 사용하도록 Airflow를 구성한 경우에도 작동합니다.</p>

<p>unittests의 구성 파일에서 이것은 미리 설정되어 있습니다.</p>

<p>프로덕션에서 사용할 다른 구성 파일에서 여기에 표시된 “핵심” 섹션에서도 유사한 설정을 찾을 수 있습니다.</p>

<p align="center"><img src="/images/post_img/data_airflow3.png" /></p>

<p><br /></p>

<h4 id="setting-up-for-production">Setting up for production</h4>

<p>기본 구성 파일에 몇 개의 폴더를 더 추가했습니다.</p>

<p>특히 “dags”폴더는 이전 강의에서 작성하는 방법을 배운 DAG를 배치하는 위치입니다.</p>

<p align="center"><img src="/images/post_img/data_airflow4.png" /></p>

<p>또한 “tests” 폴더를 확인하십시오</p>

<p>CI / CD 에서 사용할 수 있는 기능으로 채울 것입니다.</p>

<p>3 장의 데이터 변환 파이프 라인에서 했던 것처럼 파이프 라인입니다.</p>

<p>연결, 플로그인, 연결 풀 및 변수에 대한 폴더도 있습니다.</p>

<p>이 모든 기능을 통해 버전 제어하에 Airflow의 전체 배포를 쉽게 할 수 있습니다.</p>

<p><br /></p>

<h4 id="example-airflow-deployment-test">Example Airflow deployment test</h4>

<p>Airflow DAG 파일은 기술적으로 Airflow의 웹 인터페이스를 통해서만 발견되는 오류를 생성 할 수 있습니다.</p>

<p>하지만, CI / CD 파이프 라인의 테스트 단계의 일부로 이를 더 일찍 캡처 할 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">airflow.models</span> <span class="kn">import</span> <span class="n">DagBag</span>

<span class="k">def</span> <span class="nf">test_dagbag_import</span><span class="p">():</span>
  <span class="s">"""Verify that Airflow will be able to import all DAGs in the repository."""</span>
  <span class="n">dagbag</span> <span class="o">=</span> <span class="n">DagBag</span><span class="p">()</span>
  <span class="n">number_of_failures</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dagbag</span><span class="p">.</span><span class="n">import_errors</span><span class="p">)</span>
  <span class="k">assert</span> <span class="n">number_of_failures</span> <span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="s">"There should be no DAG failures. Got: %s"</span> <span class="o">%</span> <span class="n">dagbag</span><span class="p">.</span><span class="n">import_errors</span>
</code></pre></div></div>

<p>예를들어, 이것을 추가하는 훌륭한 테스트가 될 수 있습니다.</p>

<p>먼저 폴더에있는 모든 DAG의 모음 인 DagBag를 가져와 인스턴스화합니다.</p>

<p>인스턴스화되면 DAG에 대한 오류 메시지 사전을 보유합니다.</p>

<p>파이썬 구문 오류나 순환의 존재와 같은 문제가 있었습니다.</p>

<p>이 테스트에서 테스트 프레임 워크가 실패하면 CI / CD 파이프 라인이 자동 배포를 방해할 수 있습니다.</p>

<p><br /></p>

<h4 id="transferring-dags-ang-plugins">Transferring DAGS Ang plugins</h4>

<p>기본 설치가 포함 된 리포지토리에 모든 DAG를 유지하는 경우, Airflow 서버에서 저장소를 복제하여 간단히 수행 할 수 있습니다.</p>

<p>또는 DAG 파일 및 모든 종속성을 처리 코드에 가깝게 유지하는 경우, “rsync”와 같은 도구를 사용하여 DAG 파일을 서버에 복사하기 만 하면됩니다.</p>

<p>또는, 프로젝트간에 더 나은 격리를 촉진하는 압축 된 아카이브 패키지 DAG를 사용합니다.</p>

<p>이후, zip 파일을 서버에 복사해야합니다.</p>

<p>Airflow 서버가 정기적으로 동기화하도록 할 수 도 있습니다.  모든 사람이 쓰는 DAG 저장소가 있는 DAG 폴더, 이것들은 단지 몇가지 접근 방식이며 더 많은 것도 있습니다.</p>
:ET